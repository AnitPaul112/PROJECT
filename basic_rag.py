import json
import os
from dotenv import load_dotenv
import openai
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pickle

# Load environment variables
load_dotenv()

class BasicBanglaRAG:
    def __init__(self, processed_data_file='processed_data.json', embeddings_file='embeddings.pkl'):
        """Basic RAG system for Bangla PDF chatbot with vector search"""
        
        # Initialize OpenAI
        self.client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # Load processed data
        with open(processed_data_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.chunks = self.data['chunks']
        self.embeddings_file = embeddings_file
        
        # Initialize sentence transformer for multilingual support
        print("ü§ñ Loading multilingual sentence transformer...")
        self.encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Load or create embeddings
        self.embeddings = self._load_or_create_embeddings()
        
        # Banglish to Bangla transliteration mapping
        self.banglish_mapping = {
            # Characters and names
            'anupam': '‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ',
            'kalyani': '‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ',
            'mama': '‡¶Æ‡¶æ‡¶Æ‡¶æ',
            'aparichita': '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ',
            'rabindranath': '‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶•',
            'tagore': '‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞',
            
            # Common words
            'golpo': '‡¶ó‡¶≤‡ßç‡¶™',
            'charitra': '‡¶ö‡¶∞‡¶ø‡¶§‡ßç‡¶∞',
            'biyer': '‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞',
            'biye': '‡¶¨‡¶ø‡¶Ø‡¶º‡ßá',
            'shikkha': '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ',
            'nari': '‡¶®‡¶æ‡¶∞‡ßÄ',
            'meye': '‡¶Æ‡ßá‡¶Ø‡¶º‡ßá',
            'chele': '‡¶õ‡ßá‡¶≤‡ßá',
            'baba': '‡¶¨‡¶æ‡¶¨‡¶æ',
            'ma': '‡¶Æ‡¶æ',
            'maa': '‡¶Æ‡¶æ',
            'ghor': '‡¶ò‡¶∞',
            'bashay': '‡¶¨‡¶æ‡¶∏‡¶æ‡¶Ø‡¶º',
            'basha': '‡¶¨‡¶æ‡¶∏‡¶æ',
            'school': '‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤',
            'college': '‡¶ï‡¶≤‡ßá‡¶ú',
            'english': '‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø',
            'bangla': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ',
            'question': '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®',
            'proshno': '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®',
            'uttor': '‡¶â‡¶§‡ßç‡¶§‡¶∞',
            'answer': '‡¶â‡¶§‡ßç‡¶§‡¶∞',
            'ki': '‡¶ï‡¶ø',
            'kemon': '‡¶ï‡ßá‡¶Æ‡¶®',
            'keno': '‡¶ï‡ßá‡¶®',
            'kothai': '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º',
            'kokhon': '‡¶ï‡¶ñ‡¶®',
            'kar': '‡¶ï‡¶æ‡¶∞',
            'koto': '‡¶ï‡¶§',
            'kotogulo': '‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã',
            'amra': '‡¶Ü‡¶Æ‡¶∞‡¶æ',
            'tumi': '‡¶§‡ßÅ‡¶Æ‡¶ø',
            'tomar': '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞',
            'amar': '‡¶Ü‡¶Æ‡¶æ‡¶∞',
            'tar': '‡¶§‡¶æ‡¶∞',
            'oder': '‡¶ì‡¶¶‡ßá‡¶∞',
            'oder': '‡¶§‡¶æ‡¶¶‡ßá‡¶∞',
            'kore': '‡¶ï‡¶∞‡ßá',
            'korte': '‡¶ï‡¶∞‡¶§‡ßá',
            'korchi': '‡¶ï‡¶∞‡¶õ‡¶ø',
            'korbo': '‡¶ï‡¶∞‡¶¨‡ßã',
            'hobe': '‡¶π‡¶¨‡ßá',
            'hoye': '‡¶π‡¶Ø‡¶º‡ßá',
            'hoyeche': '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá',
            'ache': '‡¶Ü‡¶õ‡ßá',
            'chilo': '‡¶õ‡¶ø‡¶≤',
            'chile': '‡¶õ‡¶ø‡¶≤‡ßá',
            'chole': '‡¶ö‡¶≤‡ßá',
            'gele': '‡¶ó‡ßá‡¶≤‡ßá',
            'gelo': '‡¶ó‡ßá‡¶≤‡ßã',
            'elo': '‡¶è‡¶≤‡ßã',
            'esho': '‡¶è‡¶∏‡ßã',
            'dekho': '‡¶¶‡ßá‡¶ñ‡ßã',
            'dekhi': '‡¶¶‡ßá‡¶ñ‡¶ø',
            'bolo': '‡¶¨‡¶≤‡ßã',
            'boli': '‡¶¨‡¶≤‡¶ø',
            'bole': '‡¶¨‡¶≤‡ßá',
            'boleche': '‡¶¨‡¶≤‡ßá‡¶õ‡ßá',
            'shono': '‡¶∂‡ßã‡¶®‡ßã',
            'shuni': '‡¶∂‡ßÅ‡¶®‡¶ø',
            'age': '‡¶Ü‡¶ó‡ßá',
            'pore': '‡¶™‡¶∞‡ßá',
            'ekhon': '‡¶è‡¶ñ‡¶®',
            'kal': '‡¶ï‡¶æ‡¶≤',
            'aj': '‡¶Ü‡¶ú',
            'ajke': '‡¶Ü‡¶ú‡¶ï‡ßá',
            'valo': '‡¶≠‡¶æ‡¶≤‡ßã',
            'valobashe': '‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡ßá',
            'kharap': '‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™',
            'sundor': '‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞',
            'bhalo': '‡¶≠‡¶æ‡¶≤‡ßã',
            'kalo': '‡¶ï‡¶æ‡¶≤‡ßã',
            'shada': '‡¶∏‡¶æ‡¶¶‡¶æ',
            'lal': '‡¶≤‡¶æ‡¶≤',
            'nil': '‡¶®‡ßÄ‡¶≤',
            'holud': '‡¶π‡¶≤‡ßÅ‡¶¶',
            'sobuj': '‡¶∏‡¶¨‡ßÅ‡¶ú',
            'boro': '‡¶¨‡¶°‡¶º‡ßã',
            'choto': '‡¶õ‡ßã‡¶ü‡ßã',
            'meyer': '‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶∞',
            'cheler': '‡¶õ‡ßá‡¶≤‡ßá‡¶∞',
            'babar': '‡¶¨‡¶æ‡¶¨‡¶æ‡¶∞',
            'mayer': '‡¶Æ‡¶æ‡¶Ø‡¶º‡ßá‡¶∞',
            'family': '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞',
            'poribar': '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞',
            'shathe': '‡¶∏‡¶æ‡¶•‡ßá',
            'shomoye': '‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá',
            'shomoy': '‡¶∏‡¶Æ‡¶Ø‡¶º',
            'jaygay': '‡¶ú‡¶æ‡¶Ø‡¶º‡¶ó‡¶æ‡¶Ø‡¶º',
            'jayga': '‡¶ú‡¶æ‡¶Ø‡¶º‡¶ó‡¶æ',
            'theme': '‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º',
            'bishoy': '‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º',
            'ghotona': '‡¶ò‡¶ü‡¶®‡¶æ',
            'ghote': '‡¶ò‡¶ü‡ßá',
            'hoy': '‡¶π‡¶Ø‡¶º',
            'na': '‡¶®‡¶æ',
            'nai': '‡¶®‡¶æ‡¶á',
            'nei': '‡¶®‡ßá‡¶á',
            'eto': '‡¶è‡¶§‡ßã',
            'oi': '‡¶ì‡¶á',
            'ei': '‡¶è‡¶á',
            'shei': '‡¶∏‡ßá‡¶á',
            'je': '‡¶Ø‡ßá',
            'jar': '‡¶Ø‡¶æ‡¶∞',
            'jeta': '‡¶Ø‡ßá‡¶ü‡¶æ',
            'jegulo': '‡¶Ø‡ßá‡¶ó‡ßÅ‡¶≤‡ßã',
            'analysis': '‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£',
            'bishleshan': '‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£',
            'character': '‡¶ö‡¶∞‡¶ø‡¶§‡ßç‡¶∞',
            'importance': '‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨',
            'gurutto': '‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨',
            'love': '‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶æ',
            'valobasha': '‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶æ',
            'marriage': '‡¶¨‡¶ø‡¶¨‡¶æ‡¶π',
            'bibaho': '‡¶¨‡¶ø‡¶¨‡¶æ‡¶π',
            'society': '‡¶∏‡¶Æ‡¶æ‡¶ú',
            'shomaj': '‡¶∏‡¶Æ‡¶æ‡¶ú',
            'social': '‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï',
            'shamajik': '‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï',
            'education': '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ',
            'woman': '‡¶®‡¶æ‡¶∞‡ßÄ',
            'women': '‡¶®‡¶æ‡¶∞‡ßÄ',
            'girl': '‡¶Æ‡ßá‡¶Ø‡¶º‡ßá',
            'boy': '‡¶õ‡ßá‡¶≤‡ßá',
            'man': '‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑',
            'purush': '‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑',
            'lok': '‡¶≤‡ßã‡¶ï',
            'manush': '‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑',
            'jonnyo': '‡¶ú‡¶®‡ßç‡¶Ø',
            'jonno': '‡¶ú‡¶®‡ßç‡¶Ø',
            'main': '‡¶Æ‡ßÇ‡¶≤',
            'mul': '‡¶Æ‡ßÇ‡¶≤',
            'story': '‡¶ó‡¶≤‡ßç‡¶™',
            'kotha': '‡¶ï‡¶•‡¶æ',
            'kobitay': '‡¶ï‡¶¨‡¶ø‡¶§‡¶æ‡¶Ø‡¶º',
            'kobita': '‡¶ï‡¶¨‡¶ø‡¶§‡¶æ',
            'lekhok': '‡¶≤‡ßá‡¶ñ‡¶ï',
            'lekha': '‡¶≤‡ßá‡¶ñ‡¶æ',
            'likhe': '‡¶≤‡¶ø‡¶ñ‡ßá',
            'likhte': '‡¶≤‡¶ø‡¶ñ‡¶§‡ßá',
            'lekhen': '‡¶≤‡ßá‡¶ñ‡ßá‡¶®',
            'lekheni': '‡¶≤‡ßá‡¶ñ‡ßá‡¶®‡¶ø',
            'boi': '‡¶¨‡¶á',
            'book': '‡¶¨‡¶á',
            'page': '‡¶™‡¶æ‡¶§‡¶æ',
            'pata': '‡¶™‡¶æ‡¶§‡¶æ',
            'number': '‡¶®‡¶Æ‡ßç‡¶¨‡¶∞',
            'nombor': '‡¶®‡¶Æ‡ßç‡¶¨‡¶∞',
            'details': '‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§',
            'bistari': '‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§',
            'bishesh': '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑',
            'special': '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑',
            'important': '‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£',
            'guruttopurno': '‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£',
            'interesting': '‡¶Æ‡¶ú‡¶æ‡¶∞',
            'mojar': '‡¶Æ‡¶ú‡¶æ‡¶∞',
            'shundor': '‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞',
            'cute': '‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞',
            'smart': '‡¶ö‡¶æ‡¶≤‡¶æ‡¶ï',
            'chalak': '‡¶ö‡¶æ‡¶≤‡¶æ‡¶ï',
            'wise': '‡¶ú‡ßç‡¶û‡¶æ‡¶®‡ßÄ',
            'gyani': '‡¶ú‡ßç‡¶û‡¶æ‡¶®‡ßÄ',
            'foolish': '‡¶¨‡ßã‡¶ï‡¶æ',
            'boka': '‡¶¨‡ßã‡¶ï‡¶æ',
            'stupid': '‡¶¨‡ßã‡¶ï‡¶æ',
        }
        
        print(f"‚úÖ Vector-based RAG system ready with {len(self.chunks)} chunks")
        print(f"üî§ Banglish support enabled with {len(self.banglish_mapping)} word mappings")
        print(f"üß† Using multilingual sentence transformer for semantic search")
    
    def _load_or_create_embeddings(self):
        """Load embeddings from file or create new ones"""
        if os.path.exists(self.embeddings_file):
            print("üìÅ Loading existing embeddings...")
            with open(self.embeddings_file, 'rb') as f:
                embeddings = pickle.load(f)
            print(f"‚úÖ Loaded {len(embeddings)} embeddings from cache")
            return embeddings
        else:
            print("üîÑ Creating new embeddings... (this may take a moment)")
            texts = [chunk['text'] for chunk in self.chunks]
            embeddings = self.encoder.encode(texts, show_progress_bar=True)
            
            # Save embeddings for future use
            with open(self.embeddings_file, 'wb') as f:
                pickle.dump(embeddings, f)
            print(f"üíæ Saved {len(embeddings)} embeddings to cache")
            return embeddings
    
    def convert_banglish_to_bangla(self, text):
        """Convert banglish text to bangla for better matching"""
        # Convert to lowercase for matching
        lower_text = text.lower()
        
        # Replace banglish words with bangla equivalents
        for banglish_word, bangla_word in self.banglish_mapping.items():
            # Use word boundaries to avoid partial replacements
            pattern = r'\b' + re.escape(banglish_word) + r'\b'
            lower_text = re.sub(pattern, bangla_word, lower_text)
        
        return lower_text
    
    def find_relevant_chunks_vector(self, query, top_k=5):
        """Find relevant chunks using semantic similarity (vector search)"""
        # Convert banglish to bangla for better matching
        bangla_query = self.convert_banglish_to_bangla(query)
        
        # Use both original and converted query
        combined_query = f"{query} {bangla_query}"
        
        # Encode the query
        query_embedding = self.encoder.encode([combined_query])
        
        # Calculate cosine similarity with all chunk embeddings
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # Get top_k most similar chunks
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # Return chunks with similarity scores
        relevant_chunks = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # Minimum similarity threshold
                relevant_chunks.append({
                    **self.chunks[idx],
                    'similarity_score': float(similarities[idx])
                })
        
        return relevant_chunks
    
    def find_relevant_chunks_basic(self, query, top_k=5):
        """Find relevant chunks using simple keyword matching with banglish support"""
        # Convert banglish to bangla for better matching
        bangla_query = self.convert_banglish_to_bangla(query)
        
        # Use both original and converted query for matching
        query_words = query.lower().split() + bangla_query.split()
        
        scored_chunks = []
        for chunk in self.chunks:
            text = chunk['text'].lower()
            score = 0
            
            # Simple keyword matching
            for word in query_words:
                if len(word) > 2:  # Only consider words longer than 2 characters
                    score += text.count(word)
            
            # Bonus points for exact phrase matches
            if bangla_query.strip() in text:
                score += 5
            
            if score > 0:
                scored_chunks.append({
                    'chunk': chunk,
                    'score': score
                })
        
        # Sort by score and return top_k
        scored_chunks.sort(key=lambda x: x['score'], reverse=True)
        return [item['chunk'] for item in scored_chunks[:top_k]]
    
    def find_relevant_chunks_hybrid(self, query, top_k=5):
        """Hybrid search combining vector similarity and keyword matching"""
        # Get results from both methods
        vector_chunks = self.find_relevant_chunks_vector(query, top_k=top_k*2)
        keyword_chunks = self.find_relevant_chunks_basic(query, top_k=top_k*2)
        
        # Combine and score
        combined_chunks = {}
        
        # Add vector results with similarity scores
        for i, chunk in enumerate(vector_chunks):
            chunk_id = chunk['id']
            score = chunk.get('similarity_score', 0) * 100  # Scale up similarity
            combined_chunks[chunk_id] = {
                'chunk': chunk,
                'vector_score': score,
                'keyword_score': 0,
                'rank_bonus_vector': (len(vector_chunks) - i) * 2,
                'rank_bonus_keyword': 0
            }
        
        # Add keyword results
        for i, chunk in enumerate(keyword_chunks):
            chunk_id = chunk['id']
            if chunk_id in combined_chunks:
                # Update existing entry
                combined_chunks[chunk_id]['keyword_score'] = max(1, 10 - i)
                combined_chunks[chunk_id]['rank_bonus_keyword'] = (len(keyword_chunks) - i)
            else:
                # New entry
                combined_chunks[chunk_id] = {
                    'chunk': chunk,
                    'vector_score': 0,
                    'keyword_score': max(1, 10 - i),
                    'rank_bonus_vector': 0,
                    'rank_bonus_keyword': (len(keyword_chunks) - i)
                }
        
        # Calculate final scores
        final_chunks = []
        for chunk_data in combined_chunks.values():
            total_score = (
                chunk_data['vector_score'] * 0.6 +  # 60% weight to semantic similarity
                chunk_data['keyword_score'] * 0.3 +  # 30% weight to keyword matching
                chunk_data['rank_bonus_vector'] * 0.05 +  # 5% rank bonus from vector
                chunk_data['rank_bonus_keyword'] * 0.05   # 5% rank bonus from keyword
            )
            
            final_chunks.append({
                'chunk': chunk_data['chunk'],
                'final_score': total_score
            })
        
        # Sort by final score and return top_k
        final_chunks.sort(key=lambda x: x['final_score'], reverse=True)
        return [item['chunk'] for item in final_chunks[:top_k]]
    
    def generate_answer(self, query, relevant_chunks, conversation_history=None):
        """Generate answer using OpenAI with relevant context, banglish support, and conversation memory"""
        
        # Detect if query is banglish (mixed English-Bengali romanized)
        is_banglish = self.is_banglish_query(query)
        bangla_query = self.convert_banglish_to_bangla(query)
        
        # Prepare context from relevant chunks (Long-term memory)
        context = "\n\n".join([chunk['text'] for chunk in relevant_chunks[:3]])
        
        # Prepare conversation context (Short-term memory)
        conversation_context = ""
        if conversation_history and len(conversation_history) > 0:
            recent_conversations = conversation_history[-3:]  # Last 3 exchanges
            conversation_context = "\n\nRecent conversation context:\n"
            for i, conv in enumerate(recent_conversations, 1):
                conversation_context += f"Previous Q{i}: {conv['question']}\n"
                conversation_context += f"Previous A{i}: {conv['answer'][:100]}...\n"
        
        # Create prompt based on query type
        if is_banglish:
            prompt = f"""You are a helpful assistant for Bangla literature educational content about "‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ" (Aparichita) by Rabindranath Tagore.

The user asked in Banglish (romanized Bengali): {query}
Which translates to: {bangla_query}

LONG-TERM MEMORY (Knowledge Base):
{context}

SHORT-TERM MEMORY (Recent Conversation):
{conversation_context}

Instructions:
- The user asked in Banglish, so respond in a mix of Bangla and simple English that's easy to understand
- You can use both Bangla script and romanized Bengali (banglish) in your answer
- Consider the conversation history to provide contextual answers
- If this question relates to previous questions, mention the connection
- Be accurate and refer to the provided context
- If you cannot find the answer in the context, say so politely
- Keep answers concise but informative
- Examples: "Anupamer boyosh chilo..." or "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶õ‡¶ø‡¶≤..."

Answer:"""
        else:
            prompt = f"""You are a helpful assistant for Bangla literature educational content about "‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ" (Aparichita) by Rabindranath Tagore.

LONG-TERM MEMORY (Knowledge Base):
{context}

SHORT-TERM MEMORY (Recent Conversation):
{conversation_context}

Question: {query}

Instructions:
- Answer in the same language as the question (if Bangla question, answer in Bangla; if English question, answer in English)
- Consider the conversation history to provide contextual and connected answers
- If this question relates to previous questions, mention the connection
- Be accurate and refer to the provided context
- If you cannot find the answer in the context, say so politely
- For multiple choice questions, explain the reasoning
- Keep answers concise but informative

Answer:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant for Bangla literature education with support for Banglish (romanized Bengali) and conversation memory."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø ‡¶ò‡¶ü‡ßá‡¶õ‡ßá: {str(e)}"
    
    def is_banglish_query(self, text):
        """Detect if the query is written in banglish (romanized Bengali)"""
        # Check if text contains romanized Bengali words
        banglish_indicators = ['anupam', 'kalyani', 'mama', 'golpo', 'charitra', 'ki', 'kemon', 'keno', 
                              'tar', 'tomar', 'amar', 'ache', 'chilo', 'hoy', 'kore', 'bole', 'dekho',
                              'valo', 'bhalo', 'meyer', 'cheler', 'age', 'pore', 'ekhon']
        
        text_lower = text.lower()
        banglish_count = sum(1 for word in banglish_indicators if word in text_lower)
        
        # If more than 1 banglish word detected, consider it banglish
        return banglish_count > 0
    
    def query(self, question, search_method='hybrid', conversation_history=None):
        """Main query method with vector-based, hybrid search, and conversation memory support"""
        print(f"üîç Processing query: {question}")
        
        # Check if it's banglish and show converted query
        if self.is_banglish_query(question):
            bangla_equivalent = self.convert_banglish_to_bangla(question)
            print(f"üî§ Banglish detected, converted to: {bangla_equivalent}")
        
        # Show memory status
        if conversation_history:
            print(f"üß† Using conversation memory: {len(conversation_history)} previous exchanges")
        
        # Find relevant chunks using specified method (Long-term memory)
        if search_method == 'vector':
            print("üß† Using vector-based semantic search...")
            relevant_chunks = self.find_relevant_chunks_vector(question, top_k=5)
        elif search_method == 'keyword':
            print("üîç Using keyword-based search...")
            relevant_chunks = self.find_relevant_chunks_basic(question, top_k=5)
        else:  # hybrid (default)
            print("‚ö° Using hybrid search (vector + keyword)...")
            relevant_chunks = self.find_relevant_chunks_hybrid(question, top_k=5)
        
        if not relevant_chunks:
            return {
                'answer': "‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§\n\nSorry, answer not found for this question. Try asking differently.",
                'relevant_chunks': [],
                'success': False,
                'search_method': search_method,
                'used_conversation_memory': bool(conversation_history)
            }
        
        print(f"üìö Found {len(relevant_chunks)} relevant chunks")
        
        # Generate answer with conversation memory (Short-term + Long-term)
        answer = self.generate_answer(question, relevant_chunks, conversation_history)
        
        return {
            'answer': answer,
            'relevant_chunks': [{'text': chunk['text'], 'metadata': chunk} for chunk in relevant_chunks[:3]],
            'success': True,
            'search_method': search_method,
            'total_chunks_found': len(relevant_chunks),
            'used_conversation_memory': bool(conversation_history)
        }


