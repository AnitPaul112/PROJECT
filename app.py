import streamlit as st
from dotenv import load_dotenv
load_dotenv()
import os
api_key = os.getenv('OPENAI_API_KEY')
print(f"Loaded API Key: {api_key[:4]}...{api_key[-4:] if api_key else 'None'}")
from basic_rag import BasicBanglaRAG
from datetime import datetime

# Page config minimal
st.set_page_config(page_title="ржЕржкрж░рж┐ржЪрж┐рждрж╛", layout="centered")

st.title("ржЖрж▓рзЛржЪрзНржп ржмрж┐рж╖ржпрж╝ 'ржЕржкрж░рж┐ржЪрж┐рждрж╛' ржЧрж▓рзНржк")

# Remove API Key input and related logic

# Initialize session state for conversation memory (Short-term memory)
if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []

if 'related_topics' not in st.session_state:
    st.session_state.related_topics = set()

# Initialize or reuse RAG system (Long-term memory)
if 'rag_system' not in st.session_state:
    try:
        with st.spinner("Loading Long-term Memory (Vector Database)..."):
            st.session_state.rag_system = BasicBanglaRAG()
        st.success("Memory system loaded!")
    except Exception as e:
        st.error(f"RAG initialization failed: {e}")
        st.stop()

# Memory status sidebar
with st.sidebar:
    st.header("Memory Status")
    
    # Short-term memory
    st.subheader("Short-term Memory")
    st.metric("Conversation History", len(st.session_state.conversation_history))
    st.metric("Related Topics", len(st.session_state.related_topics))
    
    # Long-term memory
    st.subheader("Long-term Memory")
    if st.session_state.rag_system:
        st.metric("Vector Database", f"{len(st.session_state.rag_system.chunks)} chunks")
        st.info("Knowledge base loaded")
    
    # Show recent topics
    if st.session_state.related_topics:
        st.subheader("Discussed Topics")
        for topic in list(st.session_state.related_topics)[-5:]:
            st.write(f"{topic}")
    
    # Clear memory button
    if st.button("Clear Short-term Memory"):
        st.session_state.conversation_history = []
        st.session_state.related_topics = set()
        st.success("Memory cleared!")

# Input box for question
question = st.text_input(
    "ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржи", 
    placeholder="ржпрзЗржоржи: anupamer boyosh koto? ржЕржержмрж╛ ржЕржирзБржкржорзЗрж░ ржмржпрж╝рж╕ ржХржд?"
)

# Search method selection
search_method = st.selectbox(
    "рж╕рж╛рж░рзНржЪ ржкржжрзНржзрждрж┐ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рзБржи:",
    ["hybrid", "vector", "keyword"],
    format_func=lambda x: {
        "hybrid": "рж╣рж╛ржЗржмрзНрж░рж┐ржб (рж╕рзЗрж░рж╛ ржлрж▓рж╛ржлрж▓ - ржнрзЗржХрзНржЯрж░ + ржХрзАржУржпрж╝рж╛рж░рзНржб)",
        "vector": "рж╕рж┐ржорж╛ржирзНржЯрж┐ржХ рж╕рж╛рж░рзНржЪ (ржЕрж░рзНрже ржмрзБржЭрзЗ ржЦрзЛржБржЬрзЗ)",
        "keyword": "ржХрзАржУржпрж╝рж╛рж░рзНржб рж╕рж╛рж░рзНржЪ (рж╢ржмрзНржж ржорж┐рж▓)"
    }[x]
)

if st.button("ржЬрж┐ржЬрзНржЮрж╛рж╕рж╛ ржХрж░рзБржи") and question.strip():
    with st.spinner("ржЙрждрзНрждрж░ ржЦрзБржБржЬржЫрзЗ (Short-term + Long-term Memory)..."):
        # Pass conversation history for context-aware responses
        result = st.session_state.rag_system.query(
            question, 
            search_method=search_method,
            conversation_history=st.session_state.conversation_history
        )
    
    st.markdown("ржЙрждрзНрждрж░:")
    st.write(result['answer'])
    
    # Add to conversation history (Short-term memory)
    st.session_state.conversation_history.append({
        'timestamp': datetime.now().strftime("%H:%M:%S"),
        'question': question,
        'answer': result['answer'],
        'search_method': search_method
    })
    
    # Extract and add topics to related topics
    question_lower = question.lower()
    topics = []
    if 'anupam' in question_lower or 'ржЕржирзБржкржо' in question_lower:
        topics.append('ржЕржирзБржкржо')
    if 'kalyani' in question_lower or 'ржХрж▓рзНржпрж╛ржгрзА' in question_lower:
        topics.append('ржХрж▓рзНржпрж╛ржгрзА')
    if 'mama' in question_lower or 'ржорж╛ржорж╛' in question_lower:
        topics.append('ржорж╛ржорж╛')
    if 'biye' in question_lower or 'ржмрж┐ржпрж╝рзЗ' in question_lower or 'marriage' in question_lower:
        topics.append('ржмрж┐ржмрж╛рж╣')
    if 'shikkha' in question_lower or 'рж╢рж┐ржХрзНрж╖рж╛' in question_lower or 'education' in question_lower:
        topics.append('рж╢рж┐ржХрзНрж╖рж╛')
    
    for topic in topics:
        st.session_state.related_topics.add(topic)
    
    # Show search method used and memory status
    method_names = {
        "hybrid": "рж╣рж╛ржЗржмрзНрж░рж┐ржб рж╕рж╛рж░рзНржЪ",
        "vector": "рж╕рж┐ржорж╛ржирзНржЯрж┐ржХ рж╕рж╛рж░рзНржЪ", 
        "keyword": "ржХрзАржУржпрж╝рж╛рж░рзНржб рж╕рж╛рж░рзНржЪ"
    }
    
    memory_status = "Short-term + Long-term Memory" if result.get('used_conversation_memory') else "ЁЯУЪ Long-term Memory only"
    
    st.info(f"ржмрзНржпржмрж╣рзГржд ржкржжрзНржзрждрж┐: {method_names.get(result.get('search_method', 'hybrid'), 'рж╣рж╛ржЗржмрзНрж░рж┐ржб')} | {memory_status}")
    
    if result.get('relevant_chunks'):
        st.markdown("ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржпрж╕рзВрждрзНрж░:")
        for idx, chunk in enumerate(result['relevant_chunks']):
            # Show similarity score if available
            metadata = chunk.get('metadata', {})
            similarity_info = ""
            if 'similarity_score' in metadata:
                similarity_info = f" (рж╕рж┐ржорж┐рж▓рж╛рж░рж┐ржЯрж┐: {metadata['similarity_score']:.2f})"
            
            st.write(f"{idx+1}. {chunk['text'][:300]}{'...' if len(chunk['text']) > 300 else ''}{similarity_info}")

else:
    st.info("ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзЗ 'ржЬрж┐ржЬрзНржЮрж╛рж╕рж╛ ржХрж░рзБржи' ржмрж╛ржЯржирзЗ ржХрзНрж▓рж┐ржХ ржХрж░рзБржиред")
    
    # Sample questions
    st.markdown("ржиржорзБржирж╛ ржкрзНрж░рж╢рзНржи:")
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ржмрж╛ржВрж▓рж╛ржпрж╝:**")
        st.markdown("тАв ржЕржирзБржкржорзЗрж░ ржмржпрж╝рж╕ ржХржд?")
        st.markdown("тАв ржХрж▓рзНржпрж╛ржгрзАрж░ ржЪрж░рж┐рждрзНрж░ ржХрзЗржоржи?")
        st.markdown("тАв ржЧрж▓рзНржкрзЗрж░ ржорзВрж▓ ржмрж┐рж╖ржпрж╝ ржХрж┐?")
    
    with col2:
        st.markdown("**ржмрзНржпрж╛ржВрж▓рж┐рж╢рзЗ:**")
        st.markdown("тАв anupamer boyosh koto?")
        st.markdown("тАв kalyani kemon meyer chilo?")
        st.markdown("тАв golper main theme ki?")

# Show conversation history (Short-term memory)
if st.session_state.conversation_history:
    st.markdown("---")
    st.markdown("ржХржерзЛржкржХржержирзЗрж░ ржЗрждрж┐рж╣рж╛рж╕ (Short-term Memory)")
    
    # Show last 3 conversations
    recent_conversations = st.session_state.conversation_history[-3:]
    
    for i, conv in enumerate(reversed(recent_conversations), 1):
        with st.expander(f"ЁЯХР {conv['timestamp']} - {conv['question'][:50]}{'...' if len(conv['question']) > 50 else ''}"):
            st.markdown(f"**ржкрзНрж░рж╢рзНржи:** {conv['question']}")
            st.markdown(f"**ржЙрждрзНрждрж░:** {conv['answer'][:200]}{'...' if len(conv['answer']) > 200 else ''}")
            st.caption(f"ржкржжрзНржзрждрж┐: {conv['search_method']}")

# Memory explanation

